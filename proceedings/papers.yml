- abstract: Despite the widespread availability of LLMs, there remains a substantial
    gap in their capabilities and availability across diverse languages. One approach
    to address these issues has been to take an existing pre-trained LLM and continue
    to train it on new languages. While prior works have experimented with language
    adaptation, many questions around best practices and methodology have not been
    covered. In this paper, we present a comprehensive investigation into the adaptation
    of LLMs to new languages. Our study covers the key components in this process,
    including vocabulary extension, direct preference optimization and the data scarcity
    problem for human alignment in low resource languages. We scale these experiments
    across 9 languages and 2 parameter scales (7B and 70B). We compare our models
    against Llama 2, Aya-101, XGLM, BLOOM and existing language experts, outperforming
    all prior published baselines. Additionally, all evaluation code and checkpoints
    are made public to facilitate future research.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: zcc6@cornell.edu
    first_name: Zoltan
    google_scholar_id: https://scholar.google.com/citations?user=_Hh92vUAAAAJ&hl=en&oi=sra
    institution: Sambanova Systems
    last_name: Csaki
    name: Zoltan Csaki
    username: ~Zoltan_Csaki1
  - emails: bo.li@sambanovasystems.com
    first_name: Bo
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=s4bGwOAAAAAJ
    last_name: Li
    name: Bo Li
    username: ~Bo_Li43
  - emails: jonathan.li@sambanovasystems.ai
    first_name: Jonathan
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=6kk60E4AAAAJ
    last_name: Li
    middle_name: Lingjie
    name: Jonathan Lingjie Li
    username: ~Jonathan_Lingjie_Li1
  - emails: xqt0904@gmail.com
    first_name: Qiantong
    google_scholar_id: https://scholar.google.com/citations?user=EONOwy4AAAAJ&hl=en
    institution: Sambanova Systems
    last_name: Xu
    name: Qiantong Xu
    username: ~Qiantong_Xu1
  - emails: pian.pawakapan@sambanovasystems.com
    first_name: Pian
    last_name: Pawakapan
    name: Pian Pawakapan
    username: ~Pian_Pawakapan1
  - emails: leon.zhang@sambanovasystems.com
    first_name: Leon
    google_scholar_id: https://scholar.google.com/citations?user=sxNwojTcND8C&hl=en
    institution: Sambanova Systems
    last_name: Zhang
    name: Leon Zhang
    username: ~Leon_Zhang1
  - emails: yun.du@sambanovasystems.com
    first_name: Yun
    homepage: https://www.linkedin.com/in/yun-du-ba99b524/
    institution: Sambanova Systems
    last_name: Du
    name: Yun Du
    username: ~Yun_Du2
  - emails: hengyu.zhao@sambanovasystems.com
    first_name: Hengyu
    homepage: https://scholar.google.com/citations?user=7lQIAq4AAAAJ&hl=en
    institution: Sambanova Systems
    last_name: Zhao
    name: Hengyu Zhao
    username: ~Hengyu_Zhao1
  - emails: changran\_hu@berkeley.edu
    first_name: Changran
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=gba_0WoAAAAJ
    institution: Sambanova Systems, Inc
    last_name: Hu
    name: Changran Hu
    username: ~Changran_Hu1
  - dblp_id: https://dblp.org/pid/239/4246
    emails: uthakker@cs.wisc.edu
    first_name: Urmish
    google_scholar_id: https://scholar.google.com/citations?user=-GPPICQAAAAJ&hl=en
    homepage: https://urmish.github.io/
    institution: SambaNova Systems
    last_name: Thakker
    name: Urmish Thakker
    semantic_scholar_id: https://www.semanticscholar.org/author/Urmish-Thakker/70296695
    username: ~Urmish_Thakker1
  decision: MRL
  file: 1.pdf
  id: 1
  openreview_id: PXqkYMLRwt
  pdf_file: e6d5222510fa9a8b2f389e962063156892307489.pdf
  title: 'SambaLingo: Teaching Large Language Models New Languages'
- abstract: This paper investigates biases of Large Language Models (LLMs) through
    the lens of grammatical gender. Drawing inspiration from seminal works in psycholinguistics,
    particularly the study of gender's influence on language perception, we leverage
    multilingual LLMs to revisit and expand upon the foundational experiments of Boroditsky
    (2003). Employing LLMs as a novel method for examining psycholinguistic biases
    related to grammatical gender, we prompt a model to describe nouns with adjectives
    in various languages, focusing specifically on languages with grammatical gender.
    In particular, we look at adjective co-occurrences across gender and languages,
    and train a binary classifier to predict grammatical gender given adjectives an
    LLM uses to describe a noun. Surprisingly, we find that a simple classifier can
    not only predict noun gender above chance but also exhibit cross-language transferability.
    We show that while LLMs may describe words differently in different languages,
    they are biased similarly.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: viktor.mihaylov1304@gmail.com
    first_name: Viktor
    last_name: Mihaylov
    name: Viktor Mihaylov
    username: ~Viktor_Mihaylov1
  - dblp_id: https://dblp.org/pid/284/9612
    emails: suny@robots.ox.ac.uk
    first_name: Aleksandar
    google_scholar_id: https://scholar.google.com/citations?user=cGnonsQAAAAJ&hl=en
    last_name: Shtedritski
    name: Aleksandar Shtedritski
    username: ~Aleksandar_Shtedritski1
  decision: MRL
  file: 4.pdf
  id: 4
  openreview_id: EZDwcdL1h2
  pdf_file: 4decb3316daecaa98afca8c74e4268a7105794d5.pdf
  title: 'What an Elegant Bridge: Multilingual LLMs are Biased Similarly in Different
    Languages'
- abstract: Despite advancements in English-dominant generative large language models,
    further development is needed for low-resource languages to enhance global accessibility.
    The primary methods for representing these languages are monolingual and multilingual
    pretraining. Monolingual pretraining is expensive due to hardware requirements,
    and multilingual models often have uneven performance across languages. This study
    explores an alternative solution by adapting large language models, primarily
    trained on English, to low-resource languages. We assess various strategies, including
    continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary
    extension. The results show that continual training improves language comprehension,
    as reflected in perplexity scores, and task-specific tuning generally enhances
    performance of downstream tasks. However, extending the vocabulary shows no substantial
    benefits. Additionally, while larger models improve task performance with few-shot
    tuning, multilingual models perform worse than their monolingual counterparts
    when adapted.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/31/10606
    emails: cagritoraman@gmail.com
    first_name: Cagri
    google_scholar_id: https://scholar.google.com/citations?user=3iEvqKoAAAAJ
    homepage: https://www.cagritoraman.com
    institution: METU, Middle East Technical University
    last_name: Toraman
    name: Cagri Toraman
    orcid: https://orcid.org/0000-0001-6976-3258
    semantic_scholar_id: https://www.semanticscholar.org/author/Cagri-Toraman/2648640
    username: ~Cagri_Toraman1
  decision: MRL
  file: 6.pdf
  id: 6
  openreview_id: fTyYjzAabj
  pdf_file: 9d018c48d71ae2c6d767cfda3b5a7898fff98567.pdf
  title: 'Adapting Open-Source Generative Large Language Models for Low-Resource Languages:
    A Case Study for Turkish'
- abstract: The capacity and effectiveness of pre-trained multilingual models (MLMs)
    for zero-shot cross-lingual transfer is well established. However, phenomena of
    positive or negative transfer, and the effect of language choice still need to
    be fully understood, especially in the complex setting of massively multilingual
    LMs. We propose an \textit{efficient} method to study transfer language influence
    in zero-shot performance on another target language. Unlike previous work, our
    approach \textit{disentangles downstream tasks from language}, using dedicated
    adapter units. Our findings suggest that some languages do not largely affect
    others, while some languages, especially ones unseen during pre-training, can
    be extremely beneficial or detrimental for different target languages. We find
    that no transfer language is beneficial for all target languages. We do, curiously,
    observe languages previously unseen by MLMs consistently benefit from transfer
    from \textit{almost any} language. We additionally use our modular approach to
    quantify negative interference efficiently and categorize languages accordingly.
    Furthermore, we provide a list of promising transfer-target language configurations
    that consistently lead to target language performance improvements.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/245/7489
    emails: ffaisal@gmu.edu
    first_name: Fahim
    google_scholar_id: https://scholar.google.com/citations?user=4NgtQ2EAAAAJ&hl=en
    institution: ', George Mason University'
    last_name: Faisal
    name: Fahim Faisal
    username: ~Fahim_Faisal1
  - dblp_id: https://dblp.org/pid/148/9479
    emails: antonis@gmu.edu
    first_name: Antonios
    google_scholar_id: https://scholar.google.com/citations?user=g_G_SNAAAAAJ&hl=en
    homepage: http://www.cs.gmu.edu/~antonis/
    institution: Athena Research Center and George Mason University
    last_name: Anastasopoulos
    name: Antonios Anastasopoulos
    orcid: https://orcid.org/0000-0002-8544-246X
    semantic_scholar_id: https://www.semanticscholar.org/author/Antonios-Anastasopoulos/49513989
    username: ~Antonios_Anastasopoulos1
  decision: MRL
  file: 7.pdf
  id: 7
  openreview_id: dS17alTUlD
  pdf_file: 20c4eb132dfa36d1b9734ad789f9b91a90187139.pdf
  title: An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual
    Language Models
- abstract: 'Training Large Language Models (LLMs) with Reinforcement Learning from
    AI Feedback (RLAIF) aligns model outputs more closely with human preferences.
    This involves an evaluator model ranking multiple candidate responses to user
    prompts. However, the rankings from popular evaluator models such as GPT-4 can
    be inconsistent.


    We propose the Repeat Ranking method, in which we evaluate the same responses
    multiple times and train only on those responses which are consistently ranked.
    Using 2,714 training prompts in 62 languages, we generated responses from 7 top
    multilingual LLMs and had GPT-4 rank them five times each. Evaluating on MT-Bench
    chat benchmarks in six languages, our method outperformed the standard practice
    of training on all available prompts.


    Our work highlights the quality versus quantity trade-off in RLAIF dataset generation
    and offers a stackable strategy for enhancing dataset and thus model quality.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: pdev438@aucklanduni.ac.nz
    first_name: Peter
    google_scholar_id: https://scholar.google.com/citations?user=xZtX7iEAAAAJ&hl=en
    last_name: Devine
    name: Peter Devine
    orcid: https://orcid.org/0000-0002-8083-320X
    username: ~Peter_Devine1
  decision: MRL
  file: 8.pdf
  id: 8
  openreview_id: VvlKfoR816
  pdf_file: aabfff46781e4b7aab7234ec92ca3675e7c6cd36.pdf
  title: 'Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets'
- abstract: "Open source large language models (LLMs) have shown great improvements\
    \ in recent times. However, many of these models are focused solely on popular\
    \ spoken languages. \n\nWe present a high quality dataset of more than 70k prompt-response\
    \ pairs in 74 languages which consist of human generated prompts and synthetic\
    \ responses. We use this dataset to train a state-of-the-art open source English\
    \ LLM to chat multilingually.\n\nWe evaluate our model on MT-Bench chat benchmarks\
    \ in 6 languages, finding that our multilingual model outperforms previous state-of-the-art\
    \ open source LLMs across each language. We further find that training on more\
    \ multilingual data is beneficial to the performance in a chosen target language\
    \ (Japanese) compared to simply training on only data in that language.\n\nThese\
    \ results indicate the necessity of training on large amounts of high quality\
    \ multilingual data to make a more accessible LLM."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: pdev438@aucklanduni.ac.nz
    first_name: Peter
    google_scholar_id: https://scholar.google.com/citations?user=xZtX7iEAAAAJ&hl=en
    last_name: Devine
    name: Peter Devine
    orcid: https://orcid.org/0000-0002-8083-320X
    username: ~Peter_Devine1
  decision: MRL
  file: 9.pdf
  id: 9
  openreview_id: ORPQ9qLS6l
  pdf_file: ea01a6a8f3f0291d84f8d286bc27920ab7a878f2.pdf
  title: 'Tagengo: A Multilingual Chat Dataset'
- abstract: Parameter-efficient fine-tuning (PEFT) using labeled task data can significantly
    improve the performance of large language models (LLMs) on the downstream task.
    However, there are 7000 languages in the world and many of these languages lack
    labeled data for real-world language generation tasks. In this paper, we propose
    to improve zero-shot cross-lingual transfer by composing expert modules trained
    separately on language or task data. Our method composes $\textit{language}$ and
    $\textit{task}$ PEFT adapters via element-wise arithmetic operations to leverage
    unlabeled data and English labeled data. We extend our approach to cases where
    labeled data from more languages is available and propose to arithmetically compose
    PEFT adapters trained on languages related to the target. Empirical results on
    summarization demonstrate that our method is a strategy that obtains consistent
    gains using minimal training of PEFT parameters.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/178/7008
    emails: alexandra.xron@gmail.com
    first_name: Alexandra
    google_scholar_id: https://scholar.google.com/citations?user=XiwRCRIAAAAJ&hl=en
    homepage: https://alexandra-chron.github.io/
    institution: Google
    last_name: Chronopoulou
    name: Alexandra Chronopoulou
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexandra-Chronopoulou/3379701
    username: ~Alexandra_Chronopoulou1
  - dblp_id: https://dblp.org/pid/222/9866.html
    emails: jonas.pfeiffer.90@gmail.com
    first_name: Jonas
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=gGB0L4kAAAAJ
    homepage: https://pfeiffer.ai
    institution: Google DeepMind
    last_name: Pfeiffer
    name: Jonas Pfeiffer
    semantic_scholar_id: https://www.semanticscholar.org/author/Jonas-Pfeiffer/153733568
    username: ~Jonas_Pfeiffer1
  - dblp_id: https://dblp.org/pid/220/3863
    emails: joshuahmaynez@gmail.com
    first_name: Joshua
    google_scholar_id: https://scholar.google.com/citations?user=ZOYd-0oAAAAJ
    institution: Google
    last_name: Maynez
    name: Joshua Maynez
    username: ~Joshua_Maynez1
  - dblp_id: https://dblp.org/pid/14/7249
    emails: xinyiw1@cs.cmu.edu
    first_name: Xinyi
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=Fo52NKoAAAAJ
    institution: Google
    last_name: Wang
    name: Xinyi Wang
    username: ~Xinyi_Wang1
  - dblp_id: https://dblp.org/pid/186/7066.html
    emails: ruder.sebastian@gmail.com
    first_name: Sebastian
    google_scholar_id: https://scholar.google.de/citations?user=8ONXPV8AAAAJ&hl=de
    homepage: http://sebastianruder.com/
    institution: Cohere and Google
    last_name: Ruder
    name: Sebastian Ruder
    semantic_scholar_id: https://www.semanticscholar.org/author/Sebastian-Ruder/2884561
    username: ~Sebastian_Ruder2
  - dblp_id: https://dblp.org/pid/47/1236
    emails: pagrawal.ml@gmail.com
    first_name: Priyanka
    homepage: https://sites.google.com/site/priyankaagr17
    institution: 'Google Deepmind '
    last_name: Agrawal
    name: Priyanka Agrawal
    username: ~Priyanka_Agrawal1
  decision: MRL
  file: 10.pdf
  id: 10
  openreview_id: iliM22lWTd
  pdf_file: 693c2bcb81aba6e0488470f964560b0d0d9b2af9.pdf
  title: Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot
    Summarization
- abstract: This study evaluates the performance of Recurrent Neural Network (RNN)
    and Transformer models in replicating cross-language structural priming, a key
    indicator of abstract grammatical representations in human language processing.
    Focusing on Chinese-English priming, which involves two typologically distinct
    languages, we examine how these models handle the robust phenomenon of structural
    priming, where exposure to a particular sentence structure increases the likelihood
    of selecting a similar structure subsequently. Our findings indicate that transformers
    outperform RNNs in generating primed sentence structures, with accuracy rates
    that exceed 25.84% to 33. 33%. This challenges the conventional belief that human
    sentence processing primarily involves recurrent and immediate processing and
    suggests a role for cue-based retrieval mechanisms. This work contributes to our
    understanding of how computational models may reflect human cognitive processes
    across diverse language families.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: zhang.yidan@ufl.edu
    first_name: Demi
    last_name: Zhang
    name: Demi Zhang
    username: ~Demi_Zhang1
  - emails: xiaobushi@ufl.edu
    first_name: Bushi
    homepage: https://kitayamachingtak.github.io/
    last_name: Xiao
    name: Bushi Xiao
    username: ~Bushi_Xiao1
  - emails: gao.chao@ufl.edu
    first_name: Chao
    last_name: Gao
    name: Chao Gao
    username: ~Chao_Gao9
  - emails: youms@ufl.edu
    first_name: Sangpil
    institution: University of Florida
    last_name: Youm
    name: Sangpil Youm
    orcid: https://orcid.org/0000-0001-7234-0395
    semantic_scholar_id: https://www.semanticscholar.org/author/Sangpil-Youm/2189481515
    username: ~Sangpil_Youm1
  - dblp_id: https://dblp.org/pid/d/BonnieJDorr
    emails: bonniejdorr@ufl.edu
    first_name: Bonnie
    homepage: https://www.cise.ufl.edu/dorr-bonnie-j/
    institution: University of Florida
    last_name: Dorr
    middle_name: J
    name: Bonnie J Dorr
    orcid: https://orcid.org/0000-0003-4356-5813
    username: ~Bonnie_J_Dorr1
  decision: MRL
  file: 11.pdf
  id: 11
  openreview_id: GjwovP2ubS
  pdf_file: b93e992b38c31e228429a9db23a2ff6ec29d7947.pdf
  title: 'Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures
    for Cross-Language Structural Priming'
- abstract: POS tagging plays a fundamental role in numerous applications. While POS
    taggers are highly accurate in well-resourced settings, they lag behind in cases
    of limited or missing training data. This paper focuses on POS tagging for languages
    with limited data. We seek to identify favourable characteristics of datasets
    for training POS tagging models using related languages without specific training
    on the target language. This is a zero-shot approach. We investigate both mono-
    and multilingual models trained on related languages and compare their accuracies.
    Additionally, we compare these results with models trained directly on the target
    language itself. We do this for three target low-resource languages, for each
    of which we select several support languages. Our research highlights the importance
    of accurate dataset selection for developing effective zero-shot POS tagging models.
    Particularly, a strong linguistic relationship and high-quality datasets ensure
    optimal results. For extremely low-resource languages, zero-shot training proves
    to be a viable option.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: zeno.vandenbulcke@student.kuleuven.be
    first_name: Zeno
    institution: KU Leuven, KU Leuven
    last_name: Vandenbulcke
    name: Zeno Vandenbulcke
    username: ~Zeno_Vandenbulcke1
  - emails: lukas.vermeire@student.kuleuven.be
    first_name: Lukas
    last_name: Vermeire
    name: Lukas Vermeire
    username: ~Lukas_Vermeire1
  - dblp_id: https://dblp.uni-trier.de/pid/163/1873.html
    emails: miryam.delhoneux@kuleuven.be
    first_name: Miryam
    google_scholar_id: https://scholar.google.com/citations?user=Z2VK5nIAAAAJ&hl=en&oi=ao
    homepage: http://cl.lingfil.uu.se/~miryam/
    institution: KU Leuven
    last_name: De Lhoneux
    name: Miryam de Lhoneux
    orcid: https://orcid.org/0000-0001-8844-2126
    semantic_scholar_id: https://www.semanticscholar.org/author/Miryam-de-Lhoneux/3295381
    username: ~Miryam_de_Lhoneux1
  decision: MRL
  file: 13.pdf
  id: 13
  openreview_id: Xi74bmSXTT
  pdf_file: 7d1450b6881446651edfe55aa1fcba5587743544.pdf
  title: 'Recipe for Zero-shot POS Tagging: Is It Useful in Realistic Scenarios?'
#- abstract: 'While machine translation (MT) systems have seen significant improvements,
    #it is still common for translations to reflect societal biases, such as gender
    #bias. Decoder-only language models (LLMs) have demonstrated potential in MT, albeit
    #with performance slightly lagging behind traditional encoder-decoder neural machine
    #translation (NMT) systems. However, LLMs offer a unique advantage: the ability
    #to control the properties of the output through prompting. In this study, we leverage
    #this flexibility to explore Llama''s capability to produce gender-specific translations.
    #Our results indicate that Llama can generate gender-specific translations with
    #translation quality and gender bias comparable to NLLB, a state-of-the-art multilingual
    #NMT system.'
  #attributes:
    #paper_type: N/A
    #presentation_type: N/A
    #submitted_area: ''
  #authors:
  #- emails: edusanchg@gmail.com
    #first_name: Eduardo
    #institution: University College London, University of London and Meta
    #last_name: Sánchez
    #name: Eduardo Sánchez
    #orcid: https://orcid.org/0009-0001-7574-4579
    #username: ~Eduardo_Sánchez1
  #- dblp_id: https://dblp.org/pid/46/3930
    #emails: pierre.andrews@gmail.com
    #first_name: Pierre
    #google_scholar_id: https://scholar.google.com/citations?user=DiJPt0EAAAAJ&hl=en
    #last_name: Andrews
    #name: Pierre Andrews
    #orcid: https://orcid.org/0000-0001-6780-7798
    #semantic_scholar_id: https://www.semanticscholar.org/author/Pierre-Yves-Andrews/5657660
    #username: ~Pierre_Andrews1
  #- emails: pontus@stenetorp.se
    #first_name: Pontus
    #homepage: https://pontus.stenetorp.se
    #institution: University College London
    #last_name: Stenetorp
    #name: Pontus Stenetorp
    #semantic_scholar_id: https://www.semanticscholar.org/author/Pontus-Stenetorp/1918552
    #username: ~Pontus_Stenetorp1
  #- dblp_id: https://dblp.org/pid/168/0354
    #emails: mikel@reka.ai
    #first_name: Mikel
    #google_scholar_id: https://scholar.google.com/citations?user=N5InzP8AAAAJ
    #homepage: http://www.mikelartetxe.com
    #institution: Reka AI
    #last_name: Artetxe
    #name: Mikel Artetxe
    #semantic_scholar_id: https://www.semanticscholar.org/author/Mikel-Artetxe/2347956
    #username: ~Mikel_Artetxe1
  #- dblp_id: https://dblp.org/pid/17/2183
    #emails: costajussa@meta.com
    #first_name: Marta
    #google_scholar_id: https://scholar.google.com/citations?user=ESqQ7FoAAAAJ&hl=ca
    #homepage: https://www.costa-jussa.com
    #institution: Meta
    #last_name: Costa-jussà
    #middle_name: R.
    #name: Marta R. Costa-jussà
    #semantic_scholar_id: https://www.semanticscholar.org/author/Marta-R.-Costa-juss%C3%A0/1680233
    #username: ~Marta_R._Costa-jussà1
  #decision: MRL
  #file: 14.pdf
  #id: 14
  #openreview_id: rEtohkgDkv
  #pdf_file: 256a671219dcbdc863fc5cd39996dcc09d8cfb9c.pdf
  #title: Gender-specific Machine Translation with Large Language Models
- abstract: Multi-vector dense models, such as ColBERT, have proven highly effective
    in information retrieval. ColBERT's late interaction scoring approximates the
    joint query-document attention seen in cross-encoders while maintaining inference
    efficiency closer to traditional dense retrieval models, thanks to its bi-encoder
    architecture and recent optimizations in indexing and search. In this paper, we
    introduce a novel architecture and a training framework to support long context
    window and multilingual retrieval. Leveraging Matryoshka Representation Loss,
    we further demonstrate that the reducing the embedding dimensionality from 128
    to 64 has insignificant impact on the model's retrieval performance and cut storage
    requirements by up to 50%. Our new model, Jina-ColBERT-v2, demonstrates strong
    performance across a range of English and multilingual retrieval tasks,
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: han.xiao@jina.ai
    first_name: Han
    google_scholar_id: https://scholar.google.com/citations?user=jp7swwIAAAAJ&hl=en
    homepage: https://jina.ai
    institution: Jina AI
    last_name: Xiao
    name: Han Xiao
    username: ~Han_Xiao8
  - emails: bo.wang@jina.ai
    first_name: Bo
    homepage: https://bwanglzu.github.io/
    last_name: Wang
    name: Bo Wang
    username: ~Bo_Wang31
  - emails: robro612@gmail.com
    first_name: Rohan
    google_scholar_id: https://scholar.google.com/citations?user=6642fHcAAAAJ&hl=en
    last_name: Jha
    name: Rohan Jha
    orcid: https://orcid.org/0009-0001-5008-4949
    username: ~Rohan_Jha2
  decision: MRL
  file: 16.pdf
  id: 16
  openreview_id: Pm1xoZS10J
  pdf_file: fcf3f00abfda8691de065a241ec5a02635bd747b.pdf
  title: 'Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever'
- abstract: This study investigates the potential of cross-lingual transfer learning
    for Named Entity Recognition (NER) between Hindi and Nepali, two languages that,
    despite their linguistic similarities, face significant disparities in available
    resources. By leveraging multilingual BERT models, including RemBERT, BERT Multilingual,
    MuRIL, and DistilBERT Multilingual, the research examines whether pre-training
    them on a resource-rich language like Hindi can enhance NER performance in a resource-constrained
    language like Nepali and vice versa. The study conducts experiments in both monolingual
    and cross-lingual settings to evaluate the models’ effectiveness in transferring
    linguistic knowledge between the two languages. The findings reveal that while
    RemBERT and MuRIL perform well in monolingual contexts—RemBERT excelling in Hindi
    and MuRIL in Nepali—BERT Multilingual performs comparatively best in cross-lingual
    scenarios, in generalizing features across the languages. Although DistilBERT
    Multilingual demonstrates slightly lower performance in cross-lingual tasks, it
    balances efficiency with competitive results. The study underscores the importance
    of model selection based on linguistic and resource-specific contexts, highlighting
    that general-purpose models like BERT Multilingual are particularly well-suited
    for cross-lingual applications.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: dipendra.yadav@uni-greifswald.de
    first_name: Dipendra
    google_scholar_id: https://scholar.google.com/citations?user=ZzZLmJ0AAAAJ&hl=en
    last_name: Yadav
    name: Dipendra Yadav
    username: ~Dipendra_Yadav1
  - emails: sumaiya.suravee@uni-greifswald.de
    first_name: Sumaiya
    google_scholar_id: https://scholar.google.com/citations?user=YcsLg00AAAAJ&hl=en&oi=ao
    last_name: Suravee
    name: Sumaiya Suravee
    username: ~Sumaiya_Suravee1
  - emails: tobias.strauss@uni-rostock.de
    first_name: Tobias
    homepage: https://www.mathematik.uni-rostock.de/struktur/mitarbeitende/tobias-strauss/
    institution: Universität Rostock
    last_name: Strauß
    name: Tobias Strauß
    username: ~Tobias_Strauß1
  - dblp_id: https://dblp.org/pid/34/10373
    emails: kristina.yordanova@uni-greifswald.de
    first_name: Kristina
    institution: Ernst-Moritz-Arndt Universität Greifswald
    last_name: Yordanova
    name: Kristina Yordanova
    orcid: https://orcid.org/0000-0002-6428-1062
    username: ~Kristina_Yordanova1
  decision: MRL
  file: 18.pdf
  id: 18
  openreview_id: qYgu0mndh7
  pdf_file: 6ad3f1d808f09ae875c86cc7507bdce35886798e.pdf
  title: 'Cross-Lingual Named Entity Recognition for Low-Resource Languages: A Hindi-Nepali
    Case Study Using Multilingual BERT Models'
- abstract: Automatic speech recognition (ASR) for low-resource languages remains
    a challenge due to the scarcity of labeled training data. Parameter-efficient
    fine-tuning and text-only adaptation are two popular methods that have been used
    to address such low-resource settings. In this work, we investigate how these
    techniques can be effectively combined using a multilingual multimodal model like
    SeamlessM4T. Multimodal models are able to leverage unlabeled text via text-only
    adaptation with further parameter-efficient ASR fine-tuning, thus boosting ASR
    performance. We also show cross-lingual transfer from a high-resource language,
    achieving up to a relative 17% WER reduction over baseline in an extremely low-resource
    setting without any labeled speech.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: abhishekgupta@cse.iitb.ac.in
    first_name: Abhishek
    google_scholar_id: https://scholar.google.com/citations?user=46FEXfMAAAAJ&hl=en&authuser=1
    last_name: Gupta
    name: Abhishek Gupta
    username: ~Abhishek_Gupta9
  - emails: 20d070009@iitb.ac.in
    first_name: Amruta
    google_scholar_id: https://scholar.google.com/citations?user=Zz0ktaUAAAAJ&hl=en
    homepage: https://amparulekar.github.io/
    last_name: Parulekar
    name: Amruta Parulekar
    orcid: https://orcid.org/0009-0005-1760-7161
    username: ~Amruta_Parulekar1
  - emails: sameepch@iitb.ac.in
    first_name: Sameep
    google_scholar_id: https://scholar.google.com/citations?user=-lFhkOQAAAAJ&hl=en
    last_name: Chattopadhyay
    name: Sameep Chattopadhyay
    username: ~Sameep_Chattopadhyay1
  - dblp_id: https://dblp.org/pid/01/9014
    emails: preethi.iitb@gmail.com
    first_name: Preethi
    google_scholar_id: https://scholar.google.co.in/citations?user=QN_uhu8AAAAJ&hl=en
    homepage: http://www.cse.iitb.ac.in/~pjyothi
    institution: Indian Institute of Technology Bombay
    last_name: Jyothi
    name: Preethi Jyothi
    semantic_scholar_id: https://www.semanticscholar.org/author/P.-Jyothi/144859542
    username: ~Preethi_Jyothi3
  decision: MRL
  file: 19.pdf
  id: 19
  openreview_id: FOADVBOu4v
  pdf_file: 8ac2d046a8c1c484976bd947aad5241c0f9f5218.pdf
  title: Parameter-efficient Adaptation of Multilingual Multimodal Models for Low-resource
    ASR
- abstract: Previous work has explored the structure of dictionaries as directed graphs,
    with arcs between words when one word is used in the definition of another.  We
    analyze the efficacy of these methodologies and explore the cross-linguistic patterns
    of the strongly connected components of multiple monolingual dictionaries.  We
    find that the number of sources in the condensation graph of a directed dictionary
    graph is roughly stable across multiple different languages, and present future
    research directions.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: ethan.eschrich@ufl.edu
    first_name: Ethan
    institution: University of Florida
    last_name: Eschrich
    name: Ethan Eschrich
    username: ~Ethan_Eschrich1
  - dblp_id: https://dblp.org/pid/278/8017
    emails: liu.ying@ufl.edu
    first_name: Zoey
    google_scholar_id: https://scholar.google.com/citations?user=lChNEQYAAAAJ&hl=en
    homepage: https://zoeyliu18.github.io/
    institution: University of Florida
    last_name: Liu
    name: Zoey Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zoey-Liu/2109308254
    username: ~Zoey_Liu1
  decision: MRL
  file: 20.pdf
  id: 20
  openreview_id: cEB5WlWOe8
  pdf_file: 99c00b82c5acb1801d9d319ced4f2eea51d95b76.pdf
  title: Towards Cross-Linguistic Semantic Grounding using Dictionary Graph Analysis
- abstract: "There has been a surge in the development of various Large Language Models\
    \ (LLMs). However, text generation for languages other than English often faces\
    \ significant challenges, including poor generation quality and reduced computational\
    \ performance due to the disproportionate representation of tokens in the model's\
    \ vocabulary. In this work, we address these issues by developing a pipeline for\
    \ adaptation of English-oriented pre-trained models to other languages and constructing\
    \ efficient bilingual LLMs. Using this pipeline, we construct Vikhr, a state-of-the-art\
    \ bilingual open-source instruction-following LLM designed specifically for the\
    \ Russian language. \"Vikhr\" refers to the name of the Mistral LLM series and\
    \ means a \"strong gust of wind.\"\nUnlike previous Russian-language models that\
    \ typically rely on \nLoRA adapters on top of English-oriented models, sacrificing\
    \ performance for lower training costs, Vikhr features an adapted tokenizer vocabulary\
    \ and undergoes the continued pre-training and instruction tuning of all weights.\
    \ This not only enhances the model's performance but also significantly improves\
    \ its computational and contextual efficiency.\nThe remarkable performance of\
    \ Vikhr across various Russian-language benchmarks can also be attributed to our\
    \ efforts in expanding instruction datasets and corpora for continued pre-training.\
    \ Vikhr not only sets the new state of the art among open-source LLMs for Russian\
    \ but even outperforms some proprietary closed-source models on certain benchmarks.\
    \ The model weights, instruction sets, and code are publicly available."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: 411961@edu.itmo.ru
    first_name: Aleksandr
    google_scholar_id: https://scholar.google.com/citations?user=_QSdprUAAAAJ&hl=ru
    homepage: https://www.linkedin.com/in/aleksandr-nikolich-035ab4141/
    last_name: Nikolich
    name: Aleksandr Nikolich
    username: ~Aleksandr_Nikolich1
  - emails: korolevko@icloud.com
    first_name: Konstantin
    last_name: Korolev
    name: Konstantin Korolev
    orcid: https://orcid.org/0000-0002-9705-0893
    username: ~Konstantin_Korolev1
  - emails: hivaze.me@gmail.com
    first_name: Sergei
    homepage: http://github.com/hivaze
    institution: Misis
    last_name: Bratchikov
    name: Sergei Bratchikov
    username: ~Sergei_Bratchikov1
  - emails: igor.kiselev@gmail.com
    first_name: Igor
    google_scholar_id: https://scholar.google.com/citations?user=403wLvkAAAAJ&hl=en
    institution: University of Waterloo
    last_name: Kiselev
    name: Igor Kiselev
    orcid: https://orcid.org/0009-0007-5022-9443
    username: ~Igor_Kiselev2
  - dblp_id: https://dblp.org/pid/153/4473
    emails: artemshelmanov@gmail.com
    first_name: Artem
    google_scholar_id: https://scholar.google.com/citations?user=-zFR1g0AAAAJ&hl=ru&oi=ao
    homepage: https://github.com/iinemo
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Shelmanov
    name: Artem Shelmanov
    orcid: https://orcid.org/0000-0002-2151-6212
    semantic_scholar_id: https://www.semanticscholar.org/author/Artem-Shelmanov/1967424
    username: ~Artem_Shelmanov1
  decision: MRL
  file: 22.pdf
  id: 22
  openreview_id: TTQPG8jd64
  pdf_file: c24e0c6294a425fce4157789b04a7bcf27dfe419.pdf
  title: 'Vikhr: Constructing a State-of-the-art Bilingual Open-Source Instruction-Following  Large
    Language Model for Russian'
- abstract: "Approaches to improving multilingual language understanding often struggle\
    \ with significant performance gaps between high-resource and low-resource languages.\
    \ While there are efforts to align the languages in a single latent space to mitigate\
    \ such gaps, how different input-level representations influence such gaps has\
    \ not been investigated, particularly with phonemic inputs. \nWe hypothesize that\
    \ the performance gaps are affected by representation discrepancies between those\
    \ languages, and revisit the use of phonemic representations as a means to mitigate\
    \ these discrepancies.\nTo demonstrate the effectiveness of phonemic representations,\
    \ we present experiments on three representative cross-lingual tasks on 12 languages\
    \ in total. The results show that phonemic representations exhibit higher similarities\
    \ between languages compared to orthographic representations, and it consistently\
    \ outperforms grapheme-based baseline model on languages that are relatively low-resourced.\n\
    We present quantitative evidence from three cross-lingual tasks that demonstrate\
    \ the effectiveness of phonemic representations, and it is further justified by\
    \ a theoretical analysis of the cross-lingual performance gap."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: gpwl0709@korea.ac.kr
    first_name: Haeji
    last_name: Jung
    name: Haeji Jung
    username: ~Haeji_Jung1
  - dblp_id: https://dblp.org/pid/315/4736
    emails: changdae@cs.wisc.edu
    first_name: Changdae
    google_scholar_id: https://scholar.google.com/citations?user=7oAZaVcAAAAJ&hl=ko
    institution: Department of Computer Science, University of Wisconsin - Madison
    last_name: Oh
    name: Changdae Oh
    username: ~Changdae_Oh1
  - emails: jekang@sogang.ac.kr
    first_name: Jooeon
    homepage: https://github.com/jek0407
    institution: Sogang University
    last_name: Kang
    name: Jooeon Kang
    username: ~Jooeon_Kang1
  - emails: estelle26598@gm.gist.ac.kr
    first_name: Jimin
    institution: NAVER
    last_name: Sohn
    name: Jimin Sohn
    username: ~Jimin_Sohn1
  - dblp_id: https://dblp.org/pid/155/4867
    emails: kyungwoo.song@gmail.com
    first_name: Kyungwoo
    google_scholar_id: https://scholar.google.com/citations?user=HWxRii4AAAAJ&hl=ko
    homepage: https://mlai.yonsei.ac.kr
    institution: Yonsei University
    last_name: Song
    name: Kyungwoo Song
    username: ~Kyungwoo_Song1
  - dblp_id: https://dblp.org/pid/52/4354
    emails: jinkyu.kim@berkeley.edu
    first_name: Jinkyu
    homepage: https://visionai.korea.ac.kr/
    institution: Korea University
    last_name: Kim
    name: Jinkyu Kim
    username: ~Jinkyu_Kim1
  - dblp_id: https://dblp.org/pid/180/5443
    emails: dmortens@cs.cmu.edu
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?authuser=1&user=2iS5aeoAAAAJ
    homepage: http://www.cs.cmu.edu/~dmortens/
    institution: Carnegie Mellon University
    last_name: Mortensen
    middle_name: R
    name: David R Mortensen
    semantic_scholar_id: https://www.semanticscholar.org/author/David-R.-Mortensen/3407646
    username: ~David_R_Mortensen1
  decision: MRL
  file: 23.pdf
  id: 23
  openreview_id: zv3j1OzfzK
  pdf_file: 20088e59c4c7e12acd5a624ff95406b40420adc5.pdf
  title: Mitigating the Linguistic Gap with Phonemic Representations for Robust Cross-lingual
    Transfer
- abstract: '----------- EXTENDED ABSTRACT INTRODUCTION -----------


    Creole languages are low-resource languages, often genetically related to languages
    like English, French, and Portuguese, due to their linguistic histories with colonialism
    (DeGraff, 2003).  As such, Creoles stand to benefit greatly from both data-efficient
    methods and transfer-learning from high-resource languages. At the same time,
    it has been observed by Lent et al. (2022b) that machine translation (MT) is a
    highly desired language technology by speakers of many Creoles. To this end, recent
    works have contributed new datasets, allowing for the development and evaluation
    of MT systems for Creoles (Robinson et al., 2024; Lent et al. 2024). In this work,
    we explore the use of the limited monolingual and parallel data for Creoles using
    parameter-efficient adaptation methods. Specifically, we compare the performance
    of different adapter architectures over the set of available benchmarks. We find
    adapters a promising approach for Creoles because they are parameter-efficient
    and have been shown to leverage transfer learning between related languages (Faisal
    and Anastasopoulos, 2022). While we perform experiments across multiple Creoles,
    we present only on Haitian Creole in this extended abstract. For future work,
    we aim to explore the potentials for leveraging other high-resourced languages
    for parameter-efficient transfer learning.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: mrfe@cs.aau.dk
    first_name: Marcell
    homepage: https://vrmer.github.io
    last_name: Fekete
    middle_name: Richard
    name: Marcell Richard Fekete
    semantic_scholar_id: https://www.semanticscholar.org/author/Marcell-Richard-Fekete/2216221792
    username: ~Marcell_Richard_Fekete1
  - emails: elav@cs.aau.dk
    first_name: Ernests
    google_scholar_id: https://scholar.google.com/citations?user=SR-WcVYAAAAJ&hl=da
    homepage: https://ernlavr.github.io/
    last_name: Lavrinovics
    name: Ernests Lavrinovics
    orcid: https://orcid.org/0009-0000-1071-8970
    username: ~Ernests_Lavrinovics1
  - emails: nrobin38@jhu.edu
    first_name: Nathaniel
    google_scholar_id: https://scholar.google.com/citations?user=gZRDGQ4AAAAJ&hl=en
    homepage: https://n8rrobinson.wixsite.com/mysite
    institution: Department of Computer Science, Whiting School of Engineering
    last_name: Robinson
    middle_name: Romney
    name: Nathaniel Romney Robinson
    username: ~Nathaniel_Romney_Robinson1
  - dblp_id: https://dblp.org/pid/294/0802
    emails: hcle@cs.aau.dk
    first_name: Heather
    institution: Aalborg University
    last_name: Lent
    name: Heather Lent
    semantic_scholar_id: https://www.semanticscholar.org/author/Heather-Christine-Lent/49568895
    username: ~Heather_Lent1
  - dblp_id: https://dblp.org/pid/127/0168
    emails: prajdabre@gmail.com
    first_name: Raj
    google_scholar_id: https://scholar.google.co.jp/citations?user=x91u618AAAAJ&hl=en
    institution: National Institute of Information and Communications Technology (NICT),
      National Institute of Advanced Industrial Science and Technology
    last_name: Dabre
    name: Raj Dabre
    semantic_scholar_id: https://www.semanticscholar.org/author/Raj-Dabre/3209719
    username: ~Raj_Dabre1
  - dblp_id: https://dblp.org/pid/148/4464
    emails: jbjerva@cs.aau.dk
    first_name: Johannes
    google_scholar_id: https://scholar.google.com/citations?user=F9zlUBcAAAAJ
    homepage: https://vbn.aau.dk/en/persons/jbjerva
    institution: Aalborg University
    last_name: Bjerva
    name: Johannes Bjerva
    semantic_scholar_id: https://www.semanticscholar.org/author/Johannes-Bjerva/3336895
    username: ~Johannes_Bjerva1
  decision: MRL
  file: 24.pdf
  id: 24
  openreview_id: UBRxz2l4ir
  pdf_file: 4187d103f5a2f72dfb78f502e4d70dcc17586a9d.pdf
  title: Leveraging Adapters for Improved Cross-lingual Transfer for Low-Resource
    Creole MT
- abstract: 'Recent large language models (LLMs) demonstrate impressive capabilities
    in handling long contexts, some exhibiting near-perfect recall on synthetic retrieval
    tasks. However, these evaluations have mainly focused on English text and involved
    a single target sentence within lengthy contexts. Our work investigates how LLM
    performance generalizes to multilingual settings with multiple hidden target sentences.
    We create a new dataset -- {mLongRR} -- to comprehensively evaluate several multilingual
    long-context LLMs on retrieval and reasoning tasks across five languages: English,
    Vietnamese, Indonesian, Swahili, and Somali. These languages share the Latin script
    but belong to distinct language families and resource levels. Our analysis reveals
    a significant performance gap between languages. The best-performing models such
    as Gemini-1.5 and GPT-4o, achieve around 96\% accuracy in English to around 36\%
    in Somali with a single target sentence. However, this accuracy drops to 40\%
    in English and 0\% in Somali when dealing with three target sentences. Our findings
    highlight the challenges long-context LLMs face when processing longer contexts,
    an increase in the number of target sentences, or languages of lower resource
    levels.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/130/8114
    emails: ameeta@pdx.edu
    first_name: Ameeta
    google_scholar_id: https://scholar.google.com/citations?user=https://scholar.google.com/citations?hl=en&user=1XYgV6kAAAAJ&view_op=list_works&sortby=pubdate#%20your%20Google%20Scholar%20ID
    homepage: http://web.cecs.pdx.edu/~ameeta/
    institution: Portland State University
    last_name: Agrawal
    name: Ameeta Agrawal
    semantic_scholar_id: https://www.semanticscholar.org/author/Ameeta-Agrawal/2628916
    username: ~Ameeta_Agrawal1
  - emails: andang@pdx.edu
    first_name: Andy
    homepage: https://www.linkedin.com/in/andydang14/
    institution: Portland State University
    last_name: Dang
    name: Andy Dang
    username: ~Andy_Dang1
  - emails: sina.bagherinezhad@pdx.edu
    first_name: Sina
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=EuHJq20AAAAJ
    homepage: https://sinaai.github.io
    institution: Portland State University
    last_name: Bagheri Nezhad
    name: Sina Bagheri Nezhad
    semantic_scholar_id: https://www.semanticscholar.org/author/Sina-Bagheri-Nezhad/2257000051
    username: ~Sina_Bagheri_Nezhad1
  - emails: pokharel@pdx.edu
    first_name: Rhitabrat
    google_scholar_id: https://scholar.google.com/citations?user=YbmOhHUAAAAJ&hl=en&oi=ao
    last_name: Pokharel
    name: Rhitabrat Pokharel
    username: ~Rhitabrat_Pokharel1
  - emails: rschein2@pdx.edu
    first_name: Russell
    homepage: https://nlp.cs.pdx.edu/
    last_name: Scheinberg
    name: Russell Scheinberg
    username: ~Russell_Scheinberg1
  decision: MRL
  file: 25.pdf
  id: 25
  openreview_id: mGe6JiKVJN
  pdf_file: 6a8835c79c54471f737bd704c55d7ba4741e47e7.pdf
  title: Evaluating Multilingual Long-Context Models for Retrieval and Reasoning
- abstract: The development of large language models (LLMs) relies heavily on extensive,
    high-quality datasets. Publicly available datasets focus predominantly on English,
    leaving other language communities behind. To address this issue, we introduce
    Community OSCAR, a multilingual dataset initiative designed to address the gap
    between English and non-English data availability. Through a collective effort,
    Community OSCAR covers over 150 languages with 45 billion documents, totaling
    over 345 TiB of data. Initial results indicate that Community OSCAR provides valuable
    raw data for training LLMs and enhancing the performance of multilingual models.
    This work aims to contribute to the ongoing advancements in multilingual NLP and
    to support a more inclusive AI ecosystem by making high-quality, multilingual
    data more accessible to those working with low-resource languages.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/326/8265
    emails: brack@cs.tu-darmstadt.de
    first_name: Manuel
    google_scholar_id: https://scholar.google.com/citations?user=kJ9Abf8AAAAJ&hl
    institution: German Research Center for AI and Technische Universität Darmstadt
    last_name: Brack
    name: Manuel Brack
    semantic_scholar_id: https://www.semanticscholar.org/author/Manuel-Brack/2166299958
    username: ~Manuel_Brack1
  - dblp_id: https://dblp.org/pid/249/2629
    emails: malte.ostendorff@telekom.de
    first_name: Malte
    google_scholar_id: https://scholar.google.de/citations?user=8WfhSIcAAAAJ
    homepage: https://ostendorff.org
    institution: Deutsche Telekom
    last_name: Ostendorff
    name: Malte Ostendorff
    semantic_scholar_id: https://www.semanticscholar.org/author/Malte-Ostendorff/1389569281
    username: ~Malte_Ostendorff1
  - dblp_id: https://dblp.uni-trier.de/pid/254/0860
    emails: pedro@commoncrawl.org
    first_name: Pedro
    google_scholar_id: https://scholar.google.fr/citations?user=5sNdyvkAAAAJ&hl=en
    homepage: https://portizs.eu
    institution: Common Crawl Foundation
    last_name: Ortiz Suarez
    name: Pedro Ortiz Suarez
    orcid: https://orcid.org/0000-0003-0343-8852
    semantic_scholar_id: https://www.semanticscholar.org/author/Pedro-Javier-Ortiz-Suárez/147846651
    username: ~Pedro_Ortiz_Suarez1
  - emails: jose.saiz@bsc.es
    first_name: José
    google_scholar_id: https://scholar.google.es/citations?user=QGE3dxYAAAAJ&hl=en&authuser=2
    homepage: https://github.com/jsaizant
    institution: Barcelona Supercomputing Center
    last_name: Saiz
    middle_name: Javier
    name: José Javier Saiz
    orcid: https://orcid.org/0000-0002-6883-3549
    username: ~José_Javier_Saiz1
  - emails: inaki.lacunza@bsc.es
    first_name: Iñaki
    homepage: https://github.com/inakiLakunza
    institution: Barcelona Supercomputing Center
    last_name: Castilla
    middle_name: Lacunza
    name: Iñaki Lacunza Castilla
    username: ~Iñaki_Lacunza_Castilla1
  - emails: jorge.palomar@bsc.es
    first_name: Jorge
    homepage: https://www.bsc.es/es/palomar-giner-jorge
    institution: Barcelona Supercomputing Center
    last_name: Palomar-Giner
    name: Jorge Palomar-Giner
    orcid: https://orcid.org/0009-0007-4705-2130
    username: ~Jorge_Palomar-Giner1
  - dblp_id: https://dblp.org/pid/126/8727
    emails: aleksandr.shvets@bsc.es
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=BWiWj-sAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://sites.google.com/view/alexandershvets
    institution: Barcelona Supercomputing Center
    last_name: Shvets
    name: Alexander Shvets
    orcid: https://orcid.org/0000-0002-8370-2109
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexander-Shvets/34210148
    username: ~Alexander_Shvets1
  - dblp_id: https://dblp.org/pid/217/1650
    emails: schramowski@cs.tu-darmstadt.de
    first_name: Patrick
    google_scholar_id: https://scholar.google.com/citations?user=GD481RkAAAAJ&hl=en
    homepage: https://ml-research.github.io/people/pschramowski/index.html
    institution: German Research Center for AI
    last_name: Schramowski
    name: Patrick Schramowski
    orcid: https://orcid.org/0000-0003-1231-7120
    semantic_scholar_id: https://www.semanticscholar.org/author/P.-Schramowski/40896023
    username: ~Patrick_Schramowski1
  - dblp_id: https://dblp.org/pid/89/1527
    emails: georg.rehm@dfki.de
    first_name: Georg
    google_scholar_id: https://scholar.google.de/citations?user=QL0uIuMAAAAJ&hl=en
    homepage: http://georg-re.hm
    institution: Humboldt Universität Berlin and Deutsches Forschungszentrum für Künstliche
      Intelligenz
    last_name: Rehm
    name: Georg Rehm
    orcid: https://orcid.org/0000-0002-7800-1893
    semantic_scholar_id: https://www.semanticscholar.org/author/Georg-Rehm/2156673
    username: ~Georg_Rehm1
  - emails: marta.villegas@bsc.es
    first_name: Marta
    google_scholar_id: https://scholar.google.com/citations?user=lu5cBi8AAAAJ&hl=ca
    institution: Barcelona Supercomputing Center, Universitat Pompeu Fabra and Universitat
      Autònoma de Barcelona
    last_name: Villegas
    name: Marta Villegas
    orcid: https://orcid.org/0000-0003-0711-0029
    username: ~Marta_Villegas2
  - dblp_id: http://dblp.uni-trier.de/pers/hy/k/Kersting:Kristian.html
    emails: kersting@cs.tu-darmstadt.de
    first_name: Kristian
    google_scholar_id: https://scholar.google.com/citations?user=QY-earAAAAAJ&hl=en
    homepage: http://www.ml.informatik.tu-darmstadt.de/
    institution: German Research Center for AI, The Hessian Center for AI and TU Darmstadt
    last_name: Kersting
    name: Kristian Kersting
    orcid: https://orcid.org/0000-0002-2873-9152
    semantic_scholar_id: https://www.semanticscholar.org/author/K.-Kersting/1746871
    username: ~Kristian_Kersting1
  decision: MRL
  file: 27.pdf
  id: 27
  openreview_id: 0atvIP0nTe
  pdf_file: 009a5cf38cdc04bedf59d25ed1e05bbcd816631c.pdf
  title: 'Community OSCAR: A Community Effort for Multilingual Web Data'
- abstract: "Large language models (LLMs) are increasingly used in medical fields.\
    \ In mental health support, the early identification of linguistic markers associated\
    \ with mental health conditions can provide valuable support to mental health\
    \ professionals, and reduce long waiting times for patients.\nDespite the benefits\
    \ of LLMs for mental health support, there is limited research on their application\
    \ in mental health systems for languages other than English. \nOur study addresses\
    \ this gap by focusing on the detection of depression severity in Greek through\
    \ user-generated posts which are automatically translated from English. Our results\
    \ show that GPT3.5-turbo is not very successful in identifying the severity of\
    \ depression in English, and it has a varying performance in Greek as well. \n\
    Our study underscores the necessity for further research, especially in languages\
    \ with less resources.\nAlso, careful implementation is necessary to ensure that\
    \ LLMs are used effectively in mental health platforms, and human supervision\
    \ remains crucial to avoid misdiagnosis."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/171/2279
    emails: kskianis@cse.uoi.gr
    first_name: Konstantinos
    google_scholar_id: https://scholar.google.fr/citations?user=hG2ozuEAAAAJ
    homepage: https://y3nk0.github.io
    institution: University of Ioannina
    last_name: Skianis
    name: Konstantinos Skianis
    semantic_scholar_id: https://www.semanticscholar.org/author/Konstantinos-Skianis/2641961
    username: ~Konstantinos_Skianis2
  - dblp_id: https://dblp.org/pid/136/9077
    emails: a.s.dogruoz@gmail.com
    first_name: A.
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=rnwJVmAAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://www.asezadogruoz.com
    institution: Ghent University
    last_name: Doğruöz
    middle_name: Seza
    name: A. Seza Doğruöz
    semantic_scholar_id: https://www.semanticscholar.org/author/Seza-Do%C4%9Fru%C3%B6z/148034768
    username: ~A._Seza_Doğruöz1
  - dblp_id: https://dblp.org/pid/09/269
    emails: annis@aueb.gr
    first_name: John
    google_scholar_id: https://scholar.google.com/citations?user=niKjjdEAAAAJ&hl=en&oi=ao
    homepage: https://ipavlopoulos.github.io/
    institution: Athens University of Economics and Business
    last_name: Pavlopoulos
    name: John Pavlopoulos
    orcid: https://orcid.org/0000-0001-9188-7425
    semantic_scholar_id: https://www.semanticscholar.org/author/John-Pavlopoulos/2332587
    username: ~John_Pavlopoulos1
  decision: MRL
  file: 28.pdf
  id: 28
  openreview_id: q0oQipRiBt
  pdf_file: 89d665fce51c5e125ab17fd6e8017145c773709f.pdf
  title: Leveraging LLMs for Translating and Classifying Mental Health Data
- abstract: 'Large Language Models (LLMs) are becoming crucial across various fields,
    emphasizing the urgency for high-quality models in underrepresented languages.
    This study explores the unique challenges faced by low-resource languages, such
    as data scarcity, model selection, evaluation, and computational limitations,
    with a special focus on Turkish. We conduct an in-depth analysis to evaluate the
    impact of training strategies, model choices, and data availability on the performance
    of LLMs designed for underrepresented languages. Our approach includes two methodologies:
    (i) adapting existing LLMs originally pretrained in English to understand Turkish,
    and (ii) developing a model from the ground up using Turkish pretraining data,
    both supplemented with supervised fine-tuning on a novel Turkish instruction-tuning
    dataset aimed at enhancing reasoning capabilities. The relative performance of
    these methods is evaluated through the creation of a new leaderboard for Turkish
    LLMs, featuring benchmarks that assess different reasoning and knowledge skills.
    Furthermore, we conducted experiments on data and model scaling, both during pretraining
    and fine-tuning, simultaneously emphasizing the capacity for knowledge transfer
    across languages and addressing the challenges of catastrophic forgetting encountered
    during fine-tuning on a different language. Our goal is to offer a detailed guide
    for advancing the LLM framework in low-resource linguistic contexts, thereby making
    natural language processing (NLP) benefits more globally accessible.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: acikgoz2@illinois.edu
    first_name: Emre Can
    homepage: https://emrecanacikgoz.github.io/
    last_name: Acikgoz
    name: Emre Can Acikgoz
    username: ~Emre_Can_Acikgoz1
  - emails: merdogan18@ku.edu.tr
    first_name: Mete
    google_scholar_id: https://scholar.google.com/citations?user=h2nJVBgAAAAJ&hl=tr
    last_name: Erdogan
    name: Mete Erdogan
    username: ~Mete_Erdogan1
  - dblp_id: https://dblp.org/pid/84/4160
    emails: dyuret@ku.edu.tr
    first_name: Deniz
    google_scholar_id: https://scholar.google.com.tw/citations?user=EJurXJ4AAAAJ
    homepage: http://www.denizyuret.com/
    institution: Koc University
    last_name: Yuret
    name: Deniz Yuret
    username: ~Deniz_Yuret1
  decision: MRL
  file: 29.pdf
  id: 29
  openreview_id: rtrbyvf4Fm
  pdf_file: 17d028ae5ce14c5f65a954d09d1bb2e939f604ef.pdf
  title: 'Bridging the Bosphorus: Advancing Turkish Large Language Models through
    Strategies for Low-Resource Language Adaptation and Benchmarking'
- abstract: Dense retrieval systems are commonly used for information retrieval (IR).
    They rely on learning text representations through an encoder and usually require
    supervised modeling via labelled data which can be costly to obtain or simply
    unavailable. In this study, we introduce a novel unsupervised text representation
    learning technique via instruction-tuning the pre-trained encoder-decoder large
    language model (LLM) under the dual-encoder retrieval framework. We demonstrate
    on multiple languages that the corpus representation can be augmented by the representations
    of relevant synthetic queries generated by the instruct-tuned LLM founded on the
    Rao-Blackwell theorem. Furthermore, we effectively align the query and corpus
    text representation with self-instruct tuning. We evaluate our proposed method
    under low-resource settings on three English, two German and one Portuguese retrieval
    datasets measuring NDCG@10, MRR@100, Recall@100. We significantly improve the
    average zero-shot retrieval performance on all metrics, increasing out-of-box
    FLAN-T5 model variations by [4.73%, 6.15%] in absolute NDCG@10 and exceeding four
    supervised dense retrievers.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: qjz5084@psu.edu
    first_name: Qiuhai
    homepage: https://science.psu.edu/stat/people/qjz5084
    last_name: Zeng
    name: Qiuhai Zeng
    username: ~Qiuhai_Zeng1
  - dblp_id: https://dblp.org/pid/254/9268.html
    emails: chrisq626@gmail.com
    first_name: Zimeng
    google_scholar_id: https://scholar.google.com/citations?user=1bRc1F4AAAAJ&hl=en
    institution: Amazon
    last_name: Qiu
    name: Zimeng Qiu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zimeng-Qiu/1453536125
    username: ~Zimeng_Qiu1
  - dblp_id: https://dblp.org/pid/271/9010
    emails: daeyon.hwang@alumni.utoronto.ca
    first_name: Dae Yon
    google_scholar_id: https://scholar.google.com/citations?user=U3u3TUcAAAAJ&hl=ko
    homepage: https://eoduself.github.io/daeyonhwang/
    institution: Amazon AGI
    last_name: Hwang
    name: Dae Yon Hwang
    semantic_scholar_id: https://www.semanticscholar.org/author/Dae-Yon-Hwang/73652952
    username: ~Dae_Yon_Hwang1
  - emails: cynthia9109@gmail.com
    first_name: Xin
    homepage: https://www.linkedin.com/in/xin-cynthia-he-660b095a
    institution: Amazon
    last_name: He
    name: Xin He
    orcid: https://orcid.org/0009-0005-9182-6733
    username: ~Xin_He17
  - dblp_id: https://dblp.org/pid/96/4410
    emails: wcampbell@ll.mit.edu
    first_name: William
    last_name: Campbell
    middle_name: M.
    name: William M. Campbell
    username: ~William_M._Campbell1
  decision: MRL
  file: 32.pdf
  id: 32
  openreview_id: cMRfyUQ02E
  pdf_file: e39ded9b9b2b46a8bd8dacc024ec192c50160b37.pdf
  title: Unsupervised Text Representation Learning via Instruction-Tuning for Zero-Shot
    Dense Retrieval
- abstract: Language fairness in multilingual information retrieval (MLIR) systems
    is crucial for ensuring equitable access to information across diverse languages.
    This paper sheds light on the issue, based on the assumption that queries in different
    languages, but with identical semantics, should yield equivalent ranking lists
    when retrieving on the same multilingual documents. We evaluate the degree of
    fairness using both traditional retrieval methods, and a DPR neural ranker based
    on mBERT and XLM-R. Additionally, we introduce `LaKDA', a novel loss designed
    to mitigate language biases in neural MLIR approaches. Our analysis exposes intrinsic
    language biases in current MLIR technologies, with notable disparities across
    the retrieval methods, and the effectiveness of LaKDA in enhancing language fairness.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: jinryang@unimelb.edu.au
    first_name: Jinrui
    homepage: https://sites.google.com/ucsc.edu/jinruiyang/about-me
    institution: The University of Melbourne
    last_name: Yang
    name: Jinrui Yang
    username: ~Jinrui_Yang1
  - dblp_id: https://dblp.org/pid/58/3794
    emails: fan.jiang1@student.unimelb.edu.au
    first_name: Fan
    google_scholar_id: https://scholar.google.com/citations?user=87rqMLMAAAAJ&hl=en
    homepage: https://fantabulous-j.github.io/
    last_name: Jiang
    name: Fan Jiang
    semantic_scholar_id: https://www.semanticscholar.org/author/Fan-Jiang/51511290
    username: ~Fan_Jiang2
  - dblp_id: https://dblp.org/pid/65/4863
    emails: tb@ldwin.net
    first_name: Timothy
    google_scholar_id: https://scholar.google.com/citations?user=wjBD1dkAAAAJ&hl=en
    homepage: https://eltimster.github.io/www/
    institution: Mohamed bin Zayed University of Artificial Intelligence and The University
      of Melbourne
    last_name: Baldwin
    name: Timothy Baldwin
    orcid: https://orcid.org/0000-0003-4525-6950
    username: ~Timothy_Baldwin1
  decision: MRL
  file: 33.pdf
  id: 33
  openreview_id: wPVo1MgqLh
  pdf_file: 3ed390f335c1f3396fe529ede033ac1c4d14c73e.pdf
  title: 'Language Bias in Multilingual Information Retrieval: The Nature of the Beast
    and Mitigation Methods'
- abstract: In this extended abstract, we investigate the capability of Large Language
    Models (LLMs) to represent texts in multilingual contexts. Our findings reveal
    that sentence representations derived from LLMs exhibit a high degree of isomorphism
    across languages. This existing isomorphism facilitates representational alignments
    in few-shot settings. Specifically, by applying a contrastive objective at the
    representation level with only a small number (e.g., 100) of translation pairs,
    we significantly improve models' performance on Semantic Textual Similarity (STS)
    tasks across languages.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/rec/conf/emnlp/WuDLX20
    emails: d.wu@uva.nl
    first_name: Di
    google_scholar_id: https://scholar.google.com/citations?user=OyhaeJQAAAAJ&hl=en
    homepage: https://moore3930.github.io/
    institution: University of Amsterdam
    last_name: Wu
    name: Di Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Di-Wu/1405871902
    username: ~Di_Wu8
  - emails: leiyibin0816@163.com
    first_name: Yibin
    institution: University of Amsterdam
    last_name: Lei
    name: Yibin Lei
    semantic_scholar_id: https://www.semanticscholar.org/author/Yibin-Lei/2166060981
    username: ~Yibin_Lei1
  - dblp_id: https://dblp.org/pid/49/7109
    emails: andrewyates@gmail.com
    first_name: Andrew
    google_scholar_id: https://scholar.google.de/citations?user=b4ciDMsAAAAJ&hl=en
    homepage: https://andrewyates.net
    institution: University of Amsterdam
    last_name: Yates
    name: Andrew Yates
    username: ~Andrew_Yates2
  - dblp_id: https://dblp.org/pid/m/ChristofMonz
    emails: c.monz@uva.nl
    first_name: Christof
    google_scholar_id: https://scholar.google.com/citations?user=0r3PWLQAAAAJ&hl=en
    homepage: https://staff.fnwi.uva.nl/c.monz/
    institution: University of Amsterdam, University of Amsterdam
    last_name: Monz
    name: Christof Monz
    semantic_scholar_id: https://www.semanticscholar.org/author/Christof-Monz/1696402
    username: ~Christof_Monz1
  decision: MRL
  file: 34.pdf
  id: 34
  openreview_id: 8zkooY8hWr
  pdf_file: 4419e545713ea64a673e3ac67c4116c6003e1ff3.pdf
  title: Representational Isomorphism and Alignment of Multilingual Large Language
    Models
- abstract: Building robust and reliable machine learning systems requires models
    with the capacity to generalize their knowledge to interpret unseen inputs with
    different characteristics. Traditional language model evaluation tasks lack informative
    metrics about model generalization, and their applicability in new settings is
    often measured using task and language-specific downstream performance, which
    is lacking in many languages and tasks. To address this gap, we explore a set
    of efficient and reliable measures that could aid in computing more information
    related to the generalization capability of language models, particularly in cross-lingual
    zero-shot settings. Our central hypothesis is that the sharpness of a model's
    loss landscape, i.e., the representation of loss values over its weight space,
    can indicate its generalization potential, with a flatter landscape suggesting
    better generalization. We propose a novel and stable algorithm to reliably compute
    the sharpness of a model optimum, and demonstrate its correlation with successful
    cross-lingual transfer.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: sb7787@nyu.edu
    first_name: Saksham
    google_scholar_id: https://scholar.google.com/citations?user=Ekq8jsUAAAAJ&hl=en
    homepage: https://sakshambassi.github.io/
    institution: New York University
    last_name: Bassi
    name: Saksham Bassi
    orcid: https://orcid.org/0000-0003-1608-9743
    username: ~Saksham_Bassi1
  - emails: duyguataman@gmail.com
    first_name: Duygu
    google_scholar_id: https://scholar.google.co.uk/citations?user=CBcP1MQAAAAJ&hl=en&authuser=1
    homepage: https://www.duyguataman.com
    institution: New York University
    last_name: Ataman
    name: Duygu Ataman
    username: ~Duygu_Ataman2
  - dblp_id: https://dblp.org/pid/41/9736
    emails: kyunghyun.cho@nyu.edu
    first_name: Kyunghyun
    google_scholar_id: https://scholar.google.fi/citations?user=0RAmmIAAAAAJ&hl=en
    homepage: http://kyunghyuncho.me
    institution: Genentech and New York University
    last_name: Cho
    name: Kyunghyun Cho
    semantic_scholar_id: https://www.semanticscholar.org/author/Kyunghyun-Cho/1979489
    username: ~Kyunghyun_Cho1
  decision: MRL
  file: 35.pdf
  id: 35
  openreview_id: ASpACUFXyY
  pdf_file: 37a6fed6abafcbaebedff5f5be6c3b4310aab239.pdf
  title: Generalization Measures for Zero-Shot Cross-Lingual Transfer
- abstract: Most languages could be ambiguous, which means the same conveyed text
    or speech, results in different actions by different readers or listeners. In
    this project, we propose a method to detect the ambiguity of a sentence using
    translation by multilingual LLMs. In particular, we hypothesize that a good machine
    translator should preserve the ambiguity of sentences in all target languages.
    Therefore, we investigate whether ambiguity is encoded in the hidden representation
    of a translation model or, instead, if only a single meaning is encoded. In our
    experiments, we have been able to predict ambiguity of sentences with high accuracy
    using machine translation without direct use of semantics and only based on the
    reconstruction error of a function that maps the forward and backward translation
    hidden representations to each other. The potential applications of the proposed
    approach span i) detecting ambiguous sentences, ii) fine-tuning existing multilingual
    LLMs to preserve ambiguous information, and iii) developing AI systems that can
    generate ambiguity-free languages when needed.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/225/5292.html
    emails: behrang.mehrparvar@gmail.com
    first_name: Behrang
    google_scholar_id: https://scholar.google.com/citations?user=CO8EPlIAAAAJ&hl=en&oi=ao
    homepage: https://www.linkedin.com/in/bmehrparvar/
    last_name: Mehrparvar
    name: Behrang Mehrparvar
    orcid: https://orcid.org/0000-0001-8487-2022
    username: ~Behrang_Mehrparvar1
  - dblp_id: https://dblp.org/pid/182/2260
    emails: s.pezzelle@uva.nl
    first_name: Sandro
    google_scholar_id: https://scholar.google.it/citations?user=PW6eQ6YAAAAJ&hl=en&oi=ao
    homepage: https://sandropezzelle.github.io/
    institution: University of Amsterdam
    last_name: Pezzelle
    name: Sandro Pezzelle
    orcid: https://orcid.org/0000-0002-3969-7445
    semantic_scholar_id: https://www.semanticscholar.org/author/Sandro-Pezzelle/3422247
    username: ~Sandro_Pezzelle1
  decision: MRL
  file: 36.pdf
  id: 36
  openreview_id: tsymwIlBFy
  pdf_file: ceef65a970f177715e4c12b804a36a59681fa433.pdf
  title: Detecting and Translating Language Ambiguity with Multilingual LLMs
- abstract: 'This paper presents Multi-Lingual/Task Demonstration Retrieval (MLT-DR)
    for in-context learning with Large Language Models (LLMs).

    Our goal is to investigate how dense demonstration retrieval models are generalized
    across languages and tasks.

    We first convert 81 tasks into a common format, covering various languages, task
    types, and domains.

    For 8 English-based tasks among them, we use machine translation to create synthetic
    multi/cross-lingual tasks, by translating the examples into non-English languages
    to explicitly cover more than 130 languages.

    We then use an instruction-tuned LLM to estimate utility of demonstrations for
    all the tasks to train the demonstration retrieval models.

    In our experiments, we show an interesting counterintuitive observation; to compute
    embeddings of demonstrations, using both the input and ground-truth output hurts
    the generalization ability of the retriever on unseen tasks whose output space
    is quite different from those in the seen task set.

    We also examine that our retriever robustly works even with LLMs that we did not
    touch during the development of the models.

    The retrieval models'' checkpoints are publicly available at \url{URL-available-upon-publication}.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/76/2653.html
    emails: kazumah@google.com
    first_name: Kazuma
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=gVi99BIAAAAJ
    homepage: http://www.logos.t.u-tokyo.ac.jp/~hassy/
    institution: Google Research
    last_name: Hashimoto
    name: Kazuma Hashimoto
    semantic_scholar_id: https://www.semanticscholar.org/author/Kazuma-Hashimoto/20851195
    username: ~Kazuma_Hashimoto1
  - dblp_id: https://dblp.org/pid/152/3930
    emails: arjunakula@google.com
    first_name: Arjun
    google_scholar_id: https://scholar.google.com/citations?user=CNKX9bgAAAAJ&hl=en
    homepage: https://research.google/people/ArjunReddyAkula/
    institution: Google Research
    last_name: Akula
    middle_name: Reddy
    name: Arjun Reddy Akula
    semantic_scholar_id: https://www.semanticscholar.org/author/Arjun-Reddy-Akula/2947115
    username: ~Arjun_Reddy_Akula1
  - dblp_id: https://dblp.org/pid/01/7071-1
    emails: karthikraman@google.com
    first_name: Karthik
    google_scholar_id: https://scholar.google.com/citations?user=x1zTxLoAAAAJ
    institution: Google
    last_name: Raman
    name: Karthik Raman
    username: ~Karthik_Raman1
  - dblp_id: https://dblp.org/pid/80/4305
    emails: bemike@google.com
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?user=C9mxM5IAAAAJ&hl=en
    homepage: http://bendersky.github.io/
    institution: Google
    last_name: Bendersky
    name: Michael Bendersky
    orcid: https://orcid.org/0000-0002-2941-6240
    username: ~Michael_Bendersky1
  decision: MRL
  file: 37.pdf
  id: 37
  openreview_id: elgYfi7pQA
  pdf_file: 8e280a2a2fb70ffb7aabef696f2441c68ce2cad9.pdf
  title: 'MLT-DR: Multi-Lingual/Task Demonstration Retrieval\\An Attempt towards Generalized
    Retriever for In-Context Learning'
- abstract: 'We present our systems for the three tasks and five languages included
    in the MRL 2024 Shared Task on Multilingual Multi-task Information Retrieval:
    (1) Named Entity Recognition, (2) Free-form Question Answering, and (3) Multiple-choice
    Question Answering. For each task, we explored the impact of selecting different
    multilingual language models for fine-tuning across various target languages,
    and implemented an ensemble system that generates final outputs based on predictions
    from multiple fine-tuned models. All models are large language models fine-tuned
    on task-specific data. Our experimental results show that a more balanced dataset
    would yield better results. However, when training data for certain languages
    are scarce, fine-tuning on a large amount of English data supplemented by a small
    amount of ``triggering data'''' in the target language can produce decent results.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: senyu.li@mila.quebec
    first_name: Senyu
    google_scholar_id: https://scholar.google.com/citations?user=lyhV_fgAAAAJ
    last_name: Li
    name: Senyu Li
    username: ~Senyu_Li1
  - emails: hao.yu2@mail.mcgill.ca
    first_name: Hao
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=5Z10ts4AAAAJ
    homepage: https://yhpeter.github.io/
    last_name: Yu
    name: Hao Yu
    username: ~Hao_Yu15
  - emails: jessicaojo19@gmail.com
    first_name: Jessica
    institution: Lelapa AI
    last_name: Ojo
    name: Jessica Ojo
    username: ~Jessica_Ojo1
  - dblp_id: https://dblp.org/pid/230/6973
    emails: davlanade@gmail.com
    first_name: David
    google_scholar_id: https://scholar.google.ca/citations?user=W9sTkS0AAAAJ&hl=en
    homepage: https://dadelani.github.io/
    last_name: Adelani
    middle_name: Ifeoluwa
    name: David Ifeoluwa Adelani
    orcid: https://orcid.org/0000-0002-0193-2083
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Adelani/2518906
    username: ~David_Ifeoluwa_Adelani1
  decision: MRL
  file: 38.pdf
  id: 38
  openreview_id: IFI7q2UTRp
  pdf_file: 86fc1839d29ce3772badc1dd2dd2309b9756a769.pdf
  title: 'McGill NLP Group Submission to the MRL 2024 Shared Task: Ensembling Enhances
    Effectiveness of Multilingual Small LMs'
- abstract: 'We present the joint CUNI and LMU submission to the MRL~2024 Shared Task
    on Multi-lingual Multi-task Information Retrieval.

    The shared task objective was to explore how we can deploy modern methods in NLP
    in multi-lingual low-resource settings, tested on two sub-tasks: Named-entity
    recognition and question answering.

    Our solutions to the subtasks are based on data acquisition and model adaptation.

    We compare the performance of our submitted systems with the translate-test approach

    which proved to be the most useful in the previous edition of the shared task.

    Our results show that using more data as well as fine-tuning recent multilingual
    pre-trained models leads to considerable improvements over the translate-test
    baseline.

    Our code is available at https://github.com/ufal/mrl2024-multilingual-ir-shared-task.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/316/9896
    emails: haemmerl@cis.lmu.de
    first_name: Katharina
    google_scholar_id: https://scholar.google.com/citations?user=nEet7wwAAAAJ&hl=de
    last_name: Hämmerl
    name: Katharina Hämmerl
    semantic_scholar_id: https://www.semanticscholar.org/author/Katharina-H%C3%A4mmerl/2159197037
    username: ~Katharina_Hämmerl1
  - emails: manea@ufal.mff.cuni.cz
    first_name: Andrei-Alexandru
    google_scholar_id: https://scholar.google.com/citations?user=7GE7fx0AAAAJ
    last_name: Manea
    name: Andrei-Alexandru Manea
    username: ~Andrei-Alexandru_Manea1
  - emails: vico@ufal.mff.cuni.cz
    first_name: Gianluca
    google_scholar_id: https://scholar.google.com/citations?user=fCVHECcAAAAJ
    institution: Charles University Prague
    last_name: Vico
    name: Gianluca Vico
    orcid: https://orcid.org/0000-0003-0599-4983
    username: ~Gianluca_Vico1
  - emails: helcl@ufal.mff.cuni.cz
    first_name: Jindřich
    google_scholar_id: https://scholar.google.cz/citations?user=YkypSXgAAAAJ&hl=cs
    homepage: https://ufal.mff.cuni.cz/jindrich-helcl
    institution: Charles University
    last_name: Helcl
    name: Jindřich Helcl
    orcid: https://orcid.org/0000-0001-7737-3743
    semantic_scholar_id: https://www.semanticscholar.org/author/Jind%C5%99ich-Helcl/46407634
    username: ~Jindřich_Helcl1
  - dblp_id: https://dblp.org/pid/160/8774
    emails: libovicky@ufal.mff.cuni.cz
    first_name: Jindřich
    google_scholar_id: https://scholar.google.com/citations?user=47pkcSAAAAAJ
    homepage: https://ufal.mff.cuni.cz/jindrich-libovicky
    institution: Charles University Prague
    last_name: Libovický
    name: Jindřich Libovický
    orcid: https://orcid.org/0000-0001-7717-4090
    semantic_scholar_id: https://www.semanticscholar.org/author/Jind%C5%99ich-Libovick%C3%BD/3448602
    username: ~Jindřich_Libovický1
  decision: MRL
  file: 39.pdf
  id: 39
  openreview_id: 9xEhJ9heyw
  pdf_file: cb73dd5de3950eabb8a9a6270aed0fa516806433.pdf
  title: CUNI and LMU Submission to the MRL 2024 Shared Task on Multi-lingual Multi-task
    Information Retrieval

- id: 40
  authors:
    - first_name: Francesco
      last_name: Tinner
      institution: University of Amsterdam
      email: 14497425@uva.nl

    - first_name: Raghav
      last_name: Mantri
      institution: New York University
      email: raghav.mantri@nyu.edu

    - first_name: Mammad
      last_name: Hajili
      institution: Microsoft
      email: mammadhajili@microsoft.com

    - first_name: Chiamaka
      last_name: Chukwuneke
      institution: Lancaster University, UK
      email: chukwunekechiamaka3@gmail.com

    - first_name: Dylan
      last_name: Massey
      institution: University of Zurich
      email: dylan.massey@uzh.ch

    - first_name: Benjamin
      middle_name: A.
      last_name: Ajibade
      institution: University of Alabama
      email: baajibade@crimson.ua.edu

    - first_name: Bilge
      middle_name: Deniz
      last_name: Kocak
      institution: Villanova University
      email: bkocak1@villanova.edu

    - first_name: Abolade
      last_name: Dawud
      institution: Masakhane
      email: aboladedawud@gmail.com

    - first_name: Jonathan
      last_name: Atala
      institution: Anglia Ruskin University
      email: Olaatala7@gmail.com

    - first_name: Hale
      last_name: Sirin
      institution: Johns Hopkins University
      email: hsirin1@jhu.edu

    - first_name: Kayode
      last_name: Olaleye
      institution: University of Pretoria
      email: kayode.olaleye@up.ac.za

    - first_name: Anar
      last_name: Rzayev
      institution: KAIST
      email: rzayev.anar1@kaist.ac.kr

    - first_name: David
      last_name: Adelani
      institution: McGill University
      email: david.adelani@mcgill.ca

    - first_name: Duygu
      last_name: Ataman
      institution: New York University
      email: ataman@nyu.edu
  attributes:
    paper_type: long
    presentation_type: oral
    submitted_area: multilinguality
  file: 40.pdf
  title: 'Findings of the 2nd Shared Task on Multi-lingual Multi-task Information Retrieval at MRL 2024'
  abstract: 'Large language models (LLMs) demonstrate exceptional proficiency in both the comprehension and generation of textual data, particularly in English, a language for which extensive public benchmarks have been established across a wide range of natural language processing (NLP) tasks. Nonetheless, their performance in multilingual contexts and specialized domains remains less rigorously validated, raising questions about their reliability and generalizability across linguistically diverse and domain-specific settings. The second edition of the Shared Task on Multilingual Multitask Information Retrieval aims to provide a comprehensive and inclusive multilingual evaluation benchmark which aids assessing the ability of multilingual LLMs to capture logical, factual, or causal relationships within lengthy text contexts and generate language under sparse settings, particularly in scenarios with under-resourced languages. The shared task consists of two subtasks crucial to information retrieval: Named entity recognition (NER) and reading comprehension (RC), in 7 data-scarce languages: Azerbaijani, Swiss German, Turkish and \yoruba, which previously lacked annotated resources in information retrieval tasks. This year specifally focus on the multiple-choice question answering evaluation setting which provides a more objective setting for comparing different methods across languages.'
  archival: true
